# awscostintelligenceagent

docker exec -it ollama ollama run llama3  // run ollama llama model in local
